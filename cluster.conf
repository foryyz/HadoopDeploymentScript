# ============================================================
# Hadoop 3.x Distributed Cluster Config (Ubuntu 24 + VMware)
# Used by:
#   sc_all.sh     - per-node prerequisites (network/ssh/hostname)
#   sc_master.sh  - install + distribute + write configs
#   run_hadoop.sh - format/start/stop/health
# ============================================================


# =========================
# Base user
# =========================
HADOOP_USER="hadoop"
HADOOP_GROUP="hadoop"
TIMEZONE="Asia/Shanghai"


# =========================
# Network (VMware)
# =========================
NET_PREFIX="192.168.120"
NET_CIDR="24"
GATEWAY="192.168.120.2"
DNS_SERVERS=("114.114.114.114" "8.8.8.8")


# =========================
# Default IPs (used by sc_all.sh if --ip not provided)
# =========================
DEFAULT_IP_MASTER="192.168.120.10"
DEFAULT_IP_WORKER1="192.168.120.11"
DEFAULT_IP_WORKER2="192.168.120.12"


# =========================
# Hostnames
# =========================
MASTER_HOSTNAME="master"
WORKER1_HOSTNAME="worker1"
WORKER2_HOSTNAME="worker2"


# =========================
# Cluster roles
# =========================
ENABLE_SECONDARY_NAMENODE="true"
SECONDARY_NAMENODE_HOSTNAME="${WORKER1_HOSTNAME}"

ENABLE_JOBHISTORYSERVER="true"
JOBHISTORYSERVER_HOSTNAME="${MASTER_HOSTNAME}"


# =========================
# Hadoop / JDK download
# =========================
JDK_DOWNLOAD_LINK="https://mirrors.tuna.tsinghua.edu.cn/Adoptium/8/jdk/x64/linux/OpenJDK8U-jdk_x64_linux_hotspot_8u472b08.tar.gz"
HADOOP_DOWNLOAD_LINK="https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz"


# =========================
# Install paths
# =========================
INSTALL_BASE="/opt"
JAVA_DIR="${INSTALL_BASE}/jdk8"
HADOOP_DIR="${INSTALL_BASE}/hadoop"
HADOOP_SYMLINK="${INSTALL_BASE}/hadoop-current"


# =========================
# Hadoop data dirs
# =========================
HADOOP_DATA_DIR="/data/hadoop"
HDFS_NAME_DIR="${HADOOP_DATA_DIR}/namenode"
HDFS_DATA_DIR="${HADOOP_DATA_DIR}/datanode"


# =========================
# Hadoop core params
# =========================
FS_DEFAULT_PORT="9000"
HDFS_REPLICATION="2"


# =========================
# MapReduce JobHistoryServer
# =========================
MAPREDUCE_JOBHISTORY_ADDRESS_PORT="10020"
MAPREDUCE_JOBHISTORY_WEBAPP_PORT="19888"


# =========================
# SSH / distribution (hadoop@worker)
# =========================
# copy-id : interactive password input on workers (recommended)
# sshpass : non-interactive (lab use)
SSH_PUSH_MODE="copy-id"

# Only required if SSH_PUSH_MODE=sshpass
SSH_DEFAULT_PASSWORD="hadoop"


# =========================
# VMware clone identity
# =========================
FIX_CLONE_IDENTITY="true"


# =========================
# Netplan / Ubuntu network
# =========================
NETPLAN_RENDERER="networkd"


# =========================
# Cluster nodes (for /etc/hosts and workers)
# =========================
CLUSTER_HOSTNAMES=(
  "${MASTER_HOSTNAME}"
  "${WORKER1_HOSTNAME}"
  "${WORKER2_HOSTNAME}"
)

CLUSTER_IPS=(
  "${DEFAULT_IP_MASTER}"
  "${DEFAULT_IP_WORKER1}"
  "${DEFAULT_IP_WORKER2}"
)


# ============================================================
# Hadoop config file contents (sc_master.sh writes these verbatim)
# Note: variables expand when this file is sourced.
# ============================================================
CORE_SITE_XML_CONTENT=$(cat <<EOF
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://${MASTER_HOSTNAME}:${FS_DEFAULT_PORT}</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>${HADOOP_DATA_DIR}/tmp</value>
  </property>
</configuration>
EOF
)

HDFS_SITE_XML_CONTENT=$(cat <<EOF
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>${HDFS_REPLICATION}</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file://${HDFS_NAME_DIR}</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file://${HDFS_DATA_DIR}</value>
  </property>
  <property>
    <name>dfs.permissions.enabled</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
    <value>false</value>
  </property>
  <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>${SECONDARY_NAMENODE_HOSTNAME}:9868</value>
  </property>
</configuration>
EOF
)

YARN_SITE_XML_CONTENT=$(cat <<EOF
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>${MASTER_HOSTNAME}</value>
  </property>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
</configuration>
EOF
)

MAPRED_SITE_XML_CONTENT=$(cat <<EOF
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
  <property>
    <name>mapreduce.jobhistory.address</name>
    <value>${JOBHISTORYSERVER_HOSTNAME}:${MAPREDUCE_JOBHISTORY_ADDRESS_PORT}</value>
  </property>
  <property>
    <name>mapreduce.jobhistory.webapp.address</name>
    <value>${JOBHISTORYSERVER_HOSTNAME}:${MAPREDUCE_JOBHISTORY_WEBAPP_PORT}</value>
  </property>
</configuration>
EOF
)

WORKERS_FILE_CONTENT=$(cat <<EOF
${WORKER1_HOSTNAME}
${WORKER2_HOSTNAME}
EOF
)

HADOOP_ENV_SH_CONTENT=$(cat <<EOF
#!/usr/bin/env bash
export JAVA_HOME="${JAVA_DIR}"
export HADOOP_HOME="${HADOOP_SYMLINK}"
export HADOOP_CONF_DIR="\${HADOOP_HOME}/etc/hadoop"
EOF
)


# ============================================================
# Spark (Spark on YARN)
# ============================================================

# Spark download (prebuilt for Hadoop3)
SPARK_DOWNLOAD_LINK="https://mirrors.tuna.tsinghua.edu.cn/apache/spark/spark-3.5.8/spark-3.5.8-bin-hadoop3.tgz"

# Install dirs
SPARK_DIR="${INSTALL_BASE}/spark"
SPARK_SYMLINK="${INSTALL_BASE}/spark-current"

# Spark config template
SPARK_MASTER="yarn"
SPARK_DEPLOY_MODE_DEFAULT="client"   # client / cluster

# Spark event log
ENABLE_SPARK_EVENTLOG="true"
SPARK_EVENTLOG_DIR_LOCAL="/data/spark/eventlog"
SPARK_EVENTLOG_DIR_HDFS="hdfs:///spark-eventlog"

# Spark History Server (on master)
ENABLE_SPARK_HISTORY_SERVER="true"
SPARK_HISTORYSERVER_HOSTNAME="${MASTER_HOSTNAME}"
SPARK_HISTORYSERVER_UI_PORT="18080"

# Spark SQL warehouse (recommend HDFS)
SPARK_SQL_WAREHOUSE_DIR="hdfs:///spark-warehouse"
SPARK_LOCAL_METASTORE_DIR="/data/spark/metastore_db"

# Optional YARN params
# SPARK_YARN_QUEUE="default"
# SPARK_YARN_MAXAPPATTEMPTS="1"

# Optional resource defaults
# SPARK_EXECUTOR_MEMORY="1g"
# SPARK_EXECUTOR_CORES="1"
# SPARK_EXECUTOR_INSTANCES="2"
# SPARK_DRIVER_MEMORY="1g"
